# ğŸ¤– Jarvis AI Assistant

**Multimodal AI Assistant with Full macOS Control** - Your intelligent companion for Mac automation and productivity

âœ… Voice Conversation | âœ… Vision Analysis | âœ… Mac Control | âœ… Multimodal Files | âœ… Local LLM Support | âœ… Native macOS UI

---

![Swift](https://img.shields.io/badge/Swift-6.0-orange.svg)
![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)
![macOS](https://img.shields.io/badge/macOS-13.0+-black.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)

## ğŸš€ Overview

Jarvis is a cutting-edge AI assistant that combines voice conversation, computer vision, and Mac automation capabilities. Built with Swift 6/SwiftUI for the frontend and FastAPI/LangGraph for the backend, Jarvis offers a truly multimodal experience with support for local LLMs via Ollama.

## âœ¨ Key Features

### ğŸ™ï¸ **Voice Conversation Mode**
- **Natural voice interaction** with interruption handling
- **Wake word detection** - "Hey Jarvis" always listening
- **Streaming TTS** with premium voices
- **Hands-free and push-to-talk modes**
- **Context-aware conversations** with memory

### ğŸ‘ï¸ **Vision & Multimodal**
- **Screen capture analysis** - "What's on my screen?"
- **Image understanding** with GPT-5-nano vision
- **Document processing** (PDF, images, text)
- **OCR text extraction** from images
- **Real-time visual context** during conversations

### ğŸ–¥ï¸ **Mac Automation**
- **56 pre-built automation scripts**
- **App control** (open, quit, switch)
- **System settings** (volume, brightness, dark mode)
- **Browser automation** (Safari control)
- **File management** (create, open, navigate)
- **Productivity tools** (calendar, reminders, notes)

### ğŸ¤– **AI Model Support**
- **GPT-5-nano** (OpenAI) - Primary model
- **Local LLMs** via Ollama integration
- **Vision models** (LLaVA, Llama3.2-Vision)
- **Embedding models** for RAG
- **Model switching** without app restart

### ğŸ¯ **Interface Modes**
- **Focus Mode** - Always-on-top floating panel
- **Chat Mode** - Full-window conversational UI
- **Conversation Mode** - Voice-first interface
- **Unified sidebar** with conversation history

### ğŸ”’ **Safety & Privacy**
- **On-device processing** when possible
- **Blocked destructive operations**
- **No password/keychain access**
- **Local data storage** option
- **Transparent data usage**

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Swift 6 / SwiftUI Frontend                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚Chat Mode â”‚Focus Modeâ”‚Voice Conv â”‚  Vision Panel   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Vision Service | Ollama Service | Audio Pipeline â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ HTTP/WebSocket
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                FastAPI Backend Server                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚Chat API  â”‚Vision APIâ”‚Ollama APIâ”‚  WebSocket WS   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              LangGraph Orchestrator                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚OpenAI APIâ”‚Ollama LLMâ”‚Chroma DB â”‚Mac Automation   â”‚   â”‚
â”‚  â”‚GPT-5-nanoâ”‚Local     â”‚RAG Memoryâ”‚AppleScript      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- **macOS** 13.0+ (for ScreenCaptureKit and modern features)
- **Python** 3.11+
- **Xcode** 15.0+
- **OpenAI API Key** (for GPT-5-nano)
- **Ollama** (optional, for local LLMs)

### 1. Backend Setup

```bash
cd backend

# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Copy environment file
cp .env.example .env
# Edit .env with your API keys

# Start the server
python main.py
```

### 2. Frontend Setup

```bash
# Open in Xcode
open frontend/JarvisAI/JarvisAI.xcodeproj

# Build and run from Xcode (âŒ˜+R)
```

### 3. Ollama Setup (Optional)

```bash
# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Pull recommended models
ollama pull llama3.2
ollama pull llava
ollama pull all-minilm

# Start Ollama
ollama serve
```

## ğŸ“‹ Configuration

### Backend (.env)

```env
# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-5-nano

# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2

# Vision Settings
VISION_MODEL=gpt-5-nano
MAX_IMAGE_SIZE=2048

# Audio Settings
SAMPLE_RATE=16000
CHANNELS=1

# Storage
UPLOAD_DIR=./uploads
CHROMA_DB_PATH=./data/chroma
```

### Frontend (Info.plist)

```xml
<key>NSMicrophoneUsageDescription</key>
<string>Jarvis needs microphone access for voice commands</string>
<key>NSScreenCaptureDescription</key>
<string>Jarvis needs screen access for vision features</string>
```

## ğŸ¯ Usage Examples

### Voice Commands

```
"Hey Jarvis, what's the weather like?"
"Hey Jarvis, open Safari and go to apple.com"
"Hey Jarvis, set volume to 50%"
"Hey Jarvis, what's on my screen right now?"
"Hey Jarvis, create a new folder named 'Project' on desktop"
```

### Vision Features

```
# Analyze uploaded image
"Explain what's in this image"

# Screen capture
"Take a screenshot and explain what you see"

# Document analysis
"Summarize this PDF document"
"Extract text from this image"
```

### Mac Automation

```
"Open Spotify and play my liked songs"
"Create a new note with meeting summary"
"Open Terminal and navigate to project folder"
"Set up a split view with Notes and Safari"
"Take a screenshot and save to desktop"
```

## ğŸ”§ Advanced Features

### Custom Automation Scripts

Create custom AppleScript actions in `backend/services/mac_automation/scripts/`:

```applescript
-- Custom script example
on run argv
    set action to item 1 of argv
    if action is "custom_action" then
        -- Your custom logic here
        return "Action completed successfully"
    end if
end run
```

### Model Configuration

Switch between AI models dynamically:

```swift
// Use OpenAI
await ollamaService.setModel("gpt-5-nano")

// Use local model
await ollamaService.setModel("llama3.2")

// Use vision model
await ollamaService.setModel("llava")
```

### Memory Management

Configure RAG memory settings:

```python
# In backend/core/config.py
MEMORY_CONFIG = {
    "max_conversations": 1000,
    "context_window": 10000,
    "embedding_model": "all-minilm",
    "similarity_threshold": 0.7
}
```

## ğŸ› ï¸ Development

### Project Structure

```
Jarvis/
â”œâ”€â”€ backend/                 # FastAPI server
â”‚   â”œâ”€â”€ api/                # API routes
â”‚   â”‚   â”œâ”€â”€ routes/         # Endpoint definitions
â”‚   â”‚   â””â”€â”€ websocket/      # WebSocket handlers
â”‚   â”œâ”€â”€ core/               # Core utilities
â”‚   â”‚   â”œâ”€â”€ config.py       # Configuration
â”‚   â”‚   â””â”€â”€ logger.py       # Logging setup
â”‚   â”œâ”€â”€ services/           # Business logic
â”‚   â”‚   â”œâ”€â”€ mac_automation/ # AppleScript execution
â”‚   â”‚   â”œâ”€â”€ ollama.py       # Local LLM client
â”‚   â”‚   â””â”€â”€ vision.py       # Vision processing
â”‚   â””â”€â”€ agents/             # LangGraph agents
â”‚       â”œâ”€â”€ graph.py        # Agent workflow
â”‚       â””â”€â”€ tools.py        # Available tools
â”œâ”€â”€ frontend/               # Swift/SwiftUI app
â”‚   â””â”€â”€ JarvisAI/
â”‚       â”œâ”€â”€ Services/       # Network and utilities
â”‚       â”‚   â”œâ”€â”€ VisionService.swift
â”‚       â”‚   â”œâ”€â”€ OllamaService.swift
â”‚       â”‚   â””â”€â”€ ScreenCaptureService.swift
â”‚       â”œâ”€â”€ ViewModels/     # MVVM view models
â”‚       â”œâ”€â”€ Views/          # SwiftUI views
â”‚       â””â”€â”€ Models/         # Data models
â””â”€â”€ docs/                   # Documentation
```

### Adding New Features

1. **Backend**: Create new route in `api/routes/`
2. **Frontend**: Add service in `Services/`
3. **UI**: Create view in `Views/`
4. **Testing**: Add unit tests in `Tests/`

### Debug Mode

Enable debug logging:

```bash
# Backend
export LOG_LEVEL=DEBUG
python main.py

# Frontend
# In Xcode: Product â†’ Scheme â†’ Edit Scheme â†’ Run â†’ Arguments
# Add: -XCTDebugEnabled
```

## ğŸ› Troubleshooting

### Common Issues

1. **Microphone not working**
   - Check System Preferences â†’ Privacy â†’ Microphone
   - Ensure Jarvis is listed and enabled

2. **Screen capture fails**
   - Grant screen recording permission in System Preferences
   - Restart app after permission change

3. **Ollama connection error**
   - Ensure Ollama is running: `ollama serve`
   - Check if port 11434 is available

4. **Voice recognition poor**
   - Use external microphone for better quality
   - Calibrate in quiet environment

### Error Codes

| Code | Description | Solution |
|------|-------------|----------|
| E001 | Microphone permission denied | Grant microphone access |
| E002 | Screen recording denied | Grant screen recording permission |
| E003 | Ollama not connected | Start Ollama service |
| E004 | API key invalid | Check .env configuration |
| E005 | Model not found | Download required model |

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

### Development Workflow

1. Fork the repository
2. Create feature branch: `git checkout -b feature/amazing-feature`
3. Commit changes: `git commit -m 'Add amazing feature'`
4. Push to branch: `git push origin feature/amazing-feature`
5. Open Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **OpenAI** for GPT-5-nano API
- **Ollama** for local LLM support
- **Apple** for ScreenCaptureKit and AVFoundation
- **LangChain** for agent framework
- **FastAPI** for backend framework

## ğŸ“ Support

- ğŸ“§ Email: support@jarvis-ai.com
- ğŸ’¬ Discord: [Join our community](https://discord.gg/jarvis)
- ğŸ“– Docs: [jarvis-ai.com/docs](https://jarvis-ai.com/docs)

---

<div align="center">
  <p>Made with â¤ï¸ by the Jarvis Team</p>
  <p>â­ If you like this project, give us a star!</p>
</div>
